\documentclass[12pt]{article}

\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{lineno}
\usepackage{lipsum}
\usepackage[utf8]{inputenc}
\usepackage[margin=1.2in]{geometry}
\usepackage{graphicx}

\graphicspath{ {images/} }

\begin{document}

\title{Twitter Movie Sentiment}

\author{Jack Lotkowski, Gedalia Kott, Paige Nelson, Danny Shaw}

\maketitle


\noindent \textbf{ABSTRACT}: \textbf{Write a short abstract here}: \lipsum[1-1]
~\\ 
%% keywords here
\textbf{Keywords}: movie sentiment; twitter, MetaMind \textit{;}

%% main text
\section{Introduction}\label{S:1}
\textbf{Explain the problem in details and generally explain the method and results. Give proper citations here \cite{Smith:2013jd}	.} \lipsum[1-6]
\section{Method}\label{S:2}
\textbf{Explain your approach in details here. If applicable, you may illustrate the general architecture of your approach.}
\subsection{Data Sets}\label{S:3}
To test our classifier on tweets from Twitter we needed: 
\begin{itemize}
  \item Lists of movies to get tweets about
  	\begin{itemize}
  		\item 100 movies ranked as good
  		\item 100 movies ranked as bad
  		\item 100 movies that were recently released
  	\end{itemize}
  \item Tweets on each of the above  to classify
  	\begin{itemize}
  		\item 1,000 tweets per movie
  	\end{itemize}
  \item Pre-determined Rankings of the movies from the above lists for comparison with the classifiers results
\end{itemize}
This makes for a total of 300 movies and their rankings and 300,000 tweets.
\subsubsection{Movies and Rankings Data}\label{S:4}
We collected the movies from Rotten Tomatoes and the Internet Movie Database (IMDb). Rotten Tomatoes and IMDb had lists of movies ranked by categories matching our exact needs: good, bad, and recent. Our sources are included in our datasets, where we list the URLs used to obtain the movie titles and their rankings.
We scraped the HTML for the movie names and their associated scores. We then saved the data to text files in JSON format for later processing. Scores from the individual sites were in different scales. IMDb ranks movies on a scal of 1 to 10 while Rotten Tomatoes ranks movies by percentages. We normalized all scores to be on a scale of 1 to 10.
\subsubsection{Twitter Tweets Collection}
We collected 1,000 tweets per movie so we could classify the movie with our own classifier. This collection process was accomplished using the Twitter REST API (GET search/tweets). We provided the same style of query for each movie in our requests. The following is the query used to collect tweets on Ex Machina:
\begin{verbatim}
#ex_machina OR @ex_machina OR ex_machina OR #exmachina 
OR @exmachina OR exmachina OR ex machina OR Ex Machina 
...
OR "#Ex_Machina" OR "@Ex_Machina" OR "#ExMachina" 
OR "@ExMachina" OR "ExMachina"
\end{verbatim}
The code that generates the query is provided with the project. The queries were tested informally to see what would provide the best results without skewing the results for any particular movie. We found this to be the best option. Further testing and post-processing/analysis of tweets can be utilized in the future to get better tweets and remove tweets that do not actually reference the movie.
Tweets found were stored in JSON format for later consumption by the classifier.
Only recent tweets in the English language were requested being that we can only use English text and we wanted to avoid pulling popular tweets intentionally so they wouldn't skew the data.
\subsection{Classifying Tweets}\label{S:3}
In addition to gathering tweets, we also worked on creating a classifier to analyze and differentiate between tweets which had positive, negative or neutral sentiment toward a movie. 
\subsection{Wrangling Training/Testing Data}\label{S:4}
To train our classifier, we opted to use the training/testing approach to teach our classifier. Since this problem was not researched in the past, training on a nicely currated set of tweets about movies was not an option. Instead we researched different datasets related to movies and narrowed it down a dataset from the data science website Kaggle. This dataset featured 221,000 movie reviews from RottenTomatoes broken down by sentence and labeled using crowdsourcing by Amazon's Mechanical Turk. Each labeled entry ranged in length from one word to no more than a sentence, allowing our classifier to become more adept at spotting small clues present in tweets which may indicate sentiment. After writing a script to split and reformat the data we ended up with a 3 class (positive, negative and neutral), and 5 class (positive, somewhat positive, negative, somewhat negative and neutral) data files, each with 156,000 training reviews and 65,000 testing reviews. The final format was as follows:
\begin{verbatim}
3   occasionally
3   amuses but none of which amounts to much of a story
5   amuses
1   but none of which amounts to much of a story
3   but
1   none of which amounts to much of a story
3   none
\end{verbatim}
\subsection{Building the Classifier}\label
To build the classifier, we looked into a couple implementations of machine learning, focusing on NLP, to get the greatest accuracy we could: Vowpal Wabbit and MetaMind. After attempting to work with Vowpal Wabbit it was clear we needed a more approachable solution for the project given our overall lack of knowledge into the field of ML: this where MetaMind came in. MetaMind is a machine learning API developed by Stanford PhD graduate Richard Socher and developed by such minds as Ruslan Belkin, formally of Salesforce, RelateIQ and Twitter. MetaMind has done extremely well in text classification: scoring a 83.4 Pearson Correlation Coefficent Score in the SemEval 2014 competition <CITE THIS> beating out top attempts by University of Texas, Stanford and Illinois.
Building on this, we wrote an application in Python which utilized the MetaMind API to create a classifier from our RottenTomatoes training data. Testing on our testing dataset showed our classifier predicted at 67\% acurracy. 
\section{Experimental Results}\label{S:2}
\textbf{Explain the results of your study in details here. Illustrate the results in Tables and Figures and discuss the insights you obtained from them.} \lipsum[1-9]
\subsection{Classifying Tweets}\label{S:3}
To run our newely created classifier on our library of tweets we wrote a script to reformat and standardize our tweets using RegEx to remove irrelevant and non-standard complient characters. Additionally, our culminating program consolidated and fed tweets into our classifier. After a few hours, we had JSON formatted output of tweets along with our predicted sentiment formatted as follow: 
\begin{verbatim}
[
  {u'user_value': u"Furious7 was the worst movie I've ever seen. 
  	Period.", u'probability': 0.819143650919281, u'label': u'1'}, 
  {u'user_value': u'I loved Skyfall. Brilliant!', u'probability':
  	 0.9619668949957287, u'label': u'5'}
]
\end{verbatim}
With this data we were able to calculate a final metric of whether twitter users \"rated\" a movie as good (mostly positive) or bad (mostly negative).
\subsection{Analyzing Results}\label{S:3}
After collecting the necessary data and tweets, we created a webapp which can be found at http://leonardowater.herokuapp.com/twitter-sentiment. Our webapp allows users to search for a movie and after selecting a movie, the corresponding tweets are shown with positive tweets highlighted in green, negative tweets highlighted in red, and neutral tweets not highlighted. Along with the tweets, our webapp displays the Rotten Tomatoes score, our own computed Twitter Score, our computed SentiWordNet Score, as well as a dynamically-generated pie chart that visually represents the distribution of positive (green), negative (red), and neutral tweets (blue).

\includegraphics[width=\textwidth]{app}

To calculate a movie’s SentiWordNet score, we first created a hash that mapped words to their respective SentiWordNet sentiment score. Next, we iterated through the tweets of each movie and found the weighted average of all of the words in the tweet and eventually found the weighted average of the sentiment scores of the entire collection of tweets of a specific movie. Words that aren’t identified by the SentiWordNet dictionary were disregarded in our algorithm. After finding the SentiWordNet scores of every movie, we created a score distribution histogram which is shown below. Scores of 0 represent objectivity while positive scores represent positivity and negative scores negativity. Interestingly enough, the SentiWordNet scores tended to be very close to 0 with a slight edge towards positivity. Although some of the SentiWordNet scores seemed to reflect what Rotten Tomatoes and our own Twitter score showed, there did not appear to be an obvious correlation between the SentiWordNet scores which may have been attributed to the fact that many of the words in tweets were slang and not actually words which would’ve been found in the SentiWordNet word network/dictionary.

\includegraphics[width=\textwidth]{sentiword}

\bibliographystyle{plain}
\bibliography{sample}

\end{document}